{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamgoswami2711/run-your-gpt/blob/main/Gstudio_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7TVVj_z4flw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "62b0e7eb-c402-4c32-c7a7-867b1d79a162"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title ðŸŽµ Run Silent Audio Player { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ðŸ‘‡ Press play on the audio player that appears below. This will keep the Colab tab alive and prevent Google from disconnecting you for inactivity.\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWrJjouDAQHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TyU-sjMqARN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6oyrr4X0wc2",
        "outputId": "521d4bae-5256-4664-c1a9-a4bec3589a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/text-generation-webui\n",
            "Downloading the model to models/TheBloke_MythoMax-L2-13B-GPTQ\n",
            "\u001b[1;32;1m\n",
            "######################################################\n",
            "\n",
            "The model should load in about a minute. To enter TextGen, click on the link below that ends with gradio.live.\n",
            "\n",
            "For SillyTavern users, copy the \"non-streaming URL\" (ends with \"/api\") and paste it into the \"Blocking API URL\" in the API settings.\n",
            "\n",
            "You may get a \"Could not start cloudflared\" error that prevents you from receiving the API link. If you do, just stop this cell and run it again.\n",
            "\n",
            "######################################################\n",
            "\u001b[0;37;0m\n",
            "python server.py --model TheBloke_MythoMax-L2-13B-GPTQ --trust-remote-code --xformers --loader exllama --auto-devices --multi-user --api --public-api --verbose --share\n",
            "2023-08-19 14:12:05 WARNING:\u001b[33mtrust_remote_code is enabled. This is dangerous.\u001b[0m\n",
            "2023-08-19 14:12:05 WARNING:\u001b[33mThe gradio \"share link\" feature uses a proprietary executable to create a reverse tunnel. Use it with care.\u001b[0m\n",
            "2023-08-19 14:12:05 WARNING:\u001b[33mThe multi-user mode is highly experimental. DO NOT EXPOSE IT TO THE INTERNET.\u001b[0m\n",
            "2023-08-19 14:12:11.894166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-19 14:12:14 INFO:\u001b[32mLoading TheBloke_MythoMax-L2-13B-GPTQ...\u001b[0m\n",
            "2023-08-19 14:12:52 INFO:\u001b[32mReplaced attention with xformers_attention\u001b[0m\n",
            "2023-08-19 14:12:52 INFO:\u001b[32mLoaded the model in 38.02 seconds.\n",
            "\u001b[0m\n",
            "2023-08-19 14:12:52 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Starting non-streaming server at public url https://outside-jan-samples-future.trycloudflare.com/api\n",
            "Starting streaming server at public url wss://copyrights-copyrighted-helps-wild.trycloudflare.com/api/v1/stream\n",
            "Running on public URL: https://c50ccd1a40df49445e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "\n",
            "\n",
            "Hello\n",
            "--------------------\n",
            "\n",
            "Output generated in 8.68 seconds (5.99 tokens/s, 52 tokens, context 1, seed 1801251138)\n",
            "\n",
            "\n",
            "I am good too!\n",
            "--------------------\n",
            "\n",
            "Output generated in 19.61 seconds (10.20 tokens/s, 200 tokens, context 5, seed 1491080330)\n",
            "\n",
            "\n",
            "Hello\n",
            "--------------------\n",
            "\n",
            "Output generated in 14.99 seconds (8.94 tokens/s, 134 tokens, context 1, seed 860297799)\n",
            "\n",
            "\n",
            "Hello\n",
            "--------------------\n",
            "\n",
            "Output generated in 20.68 seconds (9.67 tokens/s, 200 tokens, context 1, seed 1394040575)\n",
            "\n",
            "\n",
            "Who is this?\n",
            "--------------------\n",
            "\n",
            "Output generated in 13.03 seconds (8.83 tokens/s, 115 tokens, context 4, seed 982942611)\n",
            "2023-08-19 14:19:50 ERROR:\u001b[31mconnection handler failed\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 959, in transfer_data\n",
            "    message = await self.read_message()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 1029, in read_message\n",
            "    frame = await self.read_data_frame(max_size=self.max_size)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 1104, in read_data_frame\n",
            "    frame = await self.read_frame(max_size)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 1161, in read_frame\n",
            "    frame = await Frame.read(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/framing.py\", line 68, in read\n",
            "    data = await reader(2)\n",
            "  File \"/usr/lib/python3.10/asyncio/streams.py\", line 706, in readexactly\n",
            "    raise exceptions.IncompleteReadError(incomplete, n)\n",
            "asyncio.exceptions.IncompleteReadError: 0 bytes read on a total of 2 expected bytes\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/server.py\", line 240, in handler\n",
            "    await self.ws_handler(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/server.py\", line 1186, in _ws_handler\n",
            "    return await cast(\n",
            "  File \"/content/text-generation-webui/extensions/api/streaming_api.py\", line 88, in _handle_connection\n",
            "    async for message in websocket:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 497, in __aiter__\n",
            "    yield await self.recv()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 568, in recv\n",
            "    await self.ensure_open()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/websockets/legacy/protocol.py\", line 935, in ensure_open\n",
            "    raise self.connection_closed_exc()\n",
            "websockets.exceptions.ConnectionClosedError: no close frame received or sent\n",
            "\n",
            "\n",
            "?\n",
            "--------------------\n",
            "\n",
            "Output generated in 4.98 seconds (11.05 tokens/s, 55 tokens, context 1, seed 957955683)\n",
            "\n",
            "\n",
            "What is this?\n",
            "--------------------\n",
            "\n",
            "Output generated in 15.83 seconds (9.16 tokens/s, 145 tokens, context 4, seed 1857484179)\n"
          ]
        }
      ],
      "source": [
        "#@title ##**ðŸš€ Gstudio AI API's**\n",
        "\n",
        "import sys, os, sys, base64, subprocess, json, shutil, requests, time, pathlib, multiprocessing\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files, drive\n",
        "from PIL import Image\n",
        "\n",
        "#PARAMS\n",
        "#@markdown ðŸ‘ˆ Press this button to start the installation process. The links and any further instructions will appear at the bottom after a few minutes.\n",
        "\n",
        "model_repo_download = \"TheBloke/MythoMax-L2-13B-GPTQ\"\n",
        "single_file_download = \"\"\n",
        "launch_arguments = \"\"\n",
        "save_to_google_drive = \"off\"\n",
        "verbose = True\n",
        "multi_user = True\n",
        "api = True\n",
        "superbooga = False\n",
        "google_translate = False\n",
        "long_replies = False\n",
        "character_bias = False\n",
        "silero_tts = False\n",
        "elevenlabs_tts = False\n",
        "whisper_stt = False\n",
        "send_pictures = False\n",
        "gallery = False\n",
        "sd_api_pictures = False\n",
        "openai = False\n",
        "ngrok = False\n",
        "\n",
        "settings_file = \" \"\n",
        "perplexity_colors = False\n",
        "xformers = True\n",
        "deepspeed = False\n",
        "auto_devices = True\n",
        "cpu = False\n",
        "no_cache = False\n",
        "precision = \"default\"\n",
        "wbits = \"default\"\n",
        "groupsize = \"default\"\n",
        "\n",
        "\n",
        "trust_remote_code = True\n",
        "share = True\n",
        "run_web_ui = True\n",
        "model = \"\"\n",
        "\n",
        "# Stop program if both model input fields are empty\n",
        "# You WILL download the model, and you will be happy\n",
        "if (model_repo_download == \" \".strip() and single_file_download == \" \".strip()):\n",
        "  print(f\"\\033[92m\\n\\n######################################################\\n\\nNo model selected! Please select a model above, then run this cell again.\\n\\n######################################################\\n\\n\\033[0m\")\n",
        "  sys.exit()\n",
        "\n",
        "#Install ooba\n",
        "def install_ooba():\n",
        "  global launch_arguments\n",
        "  if os.path.exists(repo_dir):\n",
        "    %cd {repo_dir}\n",
        "    !git pull\n",
        "  else:\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui.git\n",
        "  if (\"chatlogs and characters\" in save_to_google_drive):\n",
        "    if not os.path.exists(f\"{base_drive_dir}/oobabooga-data\"):\n",
        "      os.mkdir(f\"{base_drive_dir}/oobabooga-data\")\n",
        "    if not os.path.exists(f\"{base_drive_dir}/oobabooga-data/logs\"):\n",
        "      os.mkdir(f\"{base_drive_dir}/oobabooga-data/logs\")\n",
        "    if not os.path.exists(f\"{base_drive_dir}/oobabooga-data/characters\"):\n",
        "      shutil.move(\"text-generation-webui/characters\", f\"{base_drive_dir}/oobabooga-data/characters\")\n",
        "    else:\n",
        "      !rm -r \"text-generation-webui/characters\"\n",
        "\n",
        "    !ln -s \"$base_drive_dir/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "    !ln -s \"$base_drive_dir/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "  else:\n",
        "    !mkdir text-generation-webui/logs\n",
        "  !ln -s text-generation-webui/logs .\n",
        "  !ln -s text-generation-webui/characters .\n",
        "  !ln -s text-generation-webui/models .\n",
        "  %rm -r sample_data\n",
        "  %cd {repo_dir}\n",
        "  if not (\" \".strip() in settings_file):\n",
        "    !wget {settings_file} -O settings-template.yaml\n",
        "    launch_arguments.add(f'--settings settings-template.yaml')\n",
        "  !pip install -r requirements.txt | grep -v 'already satisfied'\n",
        "  print(f\"\\033[1;32;1m\\nIf you see a warning about packages, just ignore it. There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "  # Install extension req\n",
        "  if (deepspeed) or ('deepspeed' in launch_arguments):\n",
        "    !pip install -U mpi4py | grep -v 'already satisfied'\n",
        "    !pip install -U deepspeed | grep -v 'already satisfied'\n",
        "  if (xformers) or ('xformers' in launch_arguments):\n",
        "    !pip install xformers | grep -v 'already satisfied'\n",
        "  if (api) or ('api' in launch_arguments):\n",
        "    !pip install -r extensions/api/requirements.txt | grep -v 'already satisfied'\n",
        "  if (google_translate) or ('google_translate' in launch_arguments):\n",
        "    !pip install -r extensions/google_translate/requirements.txt | grep -v 'already satisfied'\n",
        "  if (superbooga) or ('superbooga' in launch_arguments):\n",
        "    !pip install -r extensions/superbooga/requirements.txt | grep -v 'already satisfied'\n",
        "  if (silero_tts) or ('silero_tts' in launch_arguments):\n",
        "    !pip install -r extensions/silero_tts/requirements.txt | grep -v 'already satisfied'\n",
        "  if (elevenlabs_tts) or ('elevenlabs_tts' in launch_arguments):\n",
        "    !pip install -r extensions/elevenlabs_tts/requirements.txt | grep -v 'already satisfied'\n",
        "  if (whisper_stt) or ('whisper_stt' in launch_arguments):\n",
        "    !pip install -r extensions/whisper_stt/requirements.txt | grep -v 'already satisfied'\n",
        "  if (openai) or ('openai' in launch_arguments):\n",
        "    !pip install -r extensions/openai/requirements.txt | grep -v 'already satisfied'\n",
        "  if (ngrok) or ('ngrok' in launch_arguments):\n",
        "    !pip install -r extensions/ngrok/requirements.txt | grep -v 'already satisfied'\n",
        "\n",
        "\n",
        "#Mount Google Drive\n",
        "if not (\"off\" in save_to_google_drive):\n",
        "  if (\"chatlogs and characters\" in save_to_google_drive):\n",
        "    drive.mount('/content/drive')\n",
        "    base_drive_dir = \"/content/drive/MyDrive/\"\n",
        "    repo_dir = '/content/text-generation-webui'\n",
        "    %cd /content\n",
        "    install_ooba()\n",
        "  if (\"chatlogs, characters, and models\" in save_to_google_drive):\n",
        "    drive.mount('/content/drive')\n",
        "    base_drive_dir = \"/content/drive/MyDrive/\"\n",
        "    repo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "    model_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "    %cd /content/drive/MyDrive\n",
        "    install_ooba()\n",
        "else:\n",
        "  %cd /content\n",
        "  repo_dir = '/content/text-generation-webui'\n",
        "  model_dir = '/content/text-generation-webui/models'\n",
        "  install_ooba()\n",
        "\n",
        "# Repo download\n",
        "def repo_download():\n",
        "  global model\n",
        "  model = model_repo_download\n",
        "  %cd {repo_dir}\n",
        "  !python download-model.py {model}\n",
        "  model = model.replace('/', '_')\n",
        "\n",
        "if (model_repo_download) and not (single_file_download):\n",
        "  clear_output(wait = False)\n",
        "  repo_download()\n",
        "\n",
        "# Single file download\n",
        "def single_download():\n",
        "  global model\n",
        "  def get_filename_from_url(single_file_download):\n",
        "    return os.path.basename(single_file_download)\n",
        "  model = get_filename_from_url(single_file_download)\n",
        "  %cd {model_dir}\n",
        "  !apt install aria2\n",
        "  !aria2c -x 16 -s 16 -o {model} {single_file_download}\n",
        "  %cd {repo_dir}\n",
        "  # Reinstall llama-cpp-python with BLAS for GGML GPU support and add llama.cpp flags\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "if (single_file_download) and not (model_repo_download):\n",
        "  clear_output(wait = False)\n",
        "  single_download()\n",
        "\n",
        "# If both input fields are filled\n",
        "if (model_repo_download and single_file_download):\n",
        "  clear_output(wait = False)\n",
        "  valid_input = False\n",
        "  while not valid_input:\n",
        "    choice = input(\"\\n\\n######################################################\\n\\nYou have selected two models at once. Choose one to download.\\n\\nIn the box below, type 1 for the repo download, or type 2 for the single file download. Then press Enter.\\n\\n\")\n",
        "    choice = int(choice)\n",
        "    if choice == 1:\n",
        "      valid_input = True\n",
        "      repo_download()\n",
        "    elif choice == 2:\n",
        "      valid_input = True\n",
        "      single_download()\n",
        "    else:\n",
        "      print(\"\\nInvalid input.\\n\")\n",
        "\n",
        "# Arguments\n",
        "launch_arguments = set()\n",
        "if ('GPTQ' in model or 'gptq' in model): launch_arguments.add('--loader exllama')\n",
        "if ('.bin' in model or 'GGML' in model or 'ggml' in model): launch_arguments.add('--threads 16 --n-gpu-layers 20 --loader llama.cpp')\n",
        "if not ('default' in wbits): launch_arguments.add(f'--wbits {wbits}')\n",
        "if not ('default' in groupsize): launch_arguments.add(f'--groupsize {groupsize}')\n",
        "if ('8bit' in precision): launch_arguments.add('--load-in-8bit')\n",
        "if ('4bit' in precision): launch_arguments.add('--load-in-4bit')\n",
        "if trust_remote_code: launch_arguments.add('--trust-remote-code')\n",
        "if multi_user: launch_arguments.add('--multi-user')\n",
        "if verbose: launch_arguments.add('--verbose')\n",
        "if no_cache: launch_arguments.add('--no-cache')\n",
        "if xformers: launch_arguments.add('--xformers')\n",
        "if deepspeed: launch_arguments.add('--deepspeed')\n",
        "if api: launch_arguments.add('--api --public-api')\n",
        "if share: launch_arguments.add('--share')\n",
        "if auto_devices: launch_arguments.add('--auto-devices')\n",
        "if cpu: launch_arguments.add('--cpu')\n",
        "\n",
        "#Extension toggles\n",
        "active_extensions = []\n",
        "if long_replies: active_extensions.append('long_replies')\n",
        "if send_pictures: active_extensions.append('send_pictures')\n",
        "if character_bias: active_extensions.append('character_bias')\n",
        "if google_translate: active_extensions.append('google_translate')\n",
        "if superbooga: active_extensions.append('superbooga')\n",
        "if silero_tts: active_extensions.append('silero_tts')\n",
        "if elevenlabs_tts: active_extensions.append('elevenlabs_tts')\n",
        "if whisper_stt: active_extensions.append('whisper_stt')\n",
        "if gallery: active_extensions.append('gallery')\n",
        "if openai: active_extensions.append('openai')\n",
        "if sd_api_pictures: active_extensions.append('sd_api_pictures')\n",
        "if ngrok: active_extensions.append('ngrok')\n",
        "if perplexity_colors: active_extensions.append('perplexity_colors')\n",
        "\n",
        "# If any extensions are selected:\n",
        "# Append the --extensions flag and all selected extensions\n",
        "if len(active_extensions) > 0:\n",
        "  launch_arguments.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "clear_output(wait = True)\n",
        "\n",
        "# Run WebUI\n",
        "print(f\"\\033[1;32;1m\\n######################################################\\n\\nThe model should load in about a minute. To enter TextGen, click on the link below that ends with gradio.live.\\n\\nFor SillyTavern users, copy the \\\"non-streaming URL\\\" (ends with \\\"/api\\\") and paste it into the \\\"Blocking API URL\\\" in the API settings.\\n\\nYou may get a \\\"Could not start cloudflared\\\" error that prevents you from receiving the API link. If you do, just stop this cell and run it again.\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "if ('deepspeed' in launch_arguments):\n",
        "  cmd =f\"deepspeed --num_gpus=1 server.py --model {model} {' '.join(launch_arguments)}\"\n",
        "  print(cmd)\n",
        "  !$cmd\n",
        "else:\n",
        "  cmd = f\"python server.py --model {model} {' '.join(launch_arguments)}\"\n",
        "  print(cmd)\n",
        "  !$cmd"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2_ai6_o1GmS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}